var documenterSearchIndex = {"docs":
[{"location":"client/#Client-Initialization","page":"Client Initialization","title":"Client Initialization","text":"","category":"section"},{"location":"client/","page":"Client Initialization","title":"Client Initialization","text":"The GithubModelsClient is the main entry point for interacting with GitHub's AI Models API.","category":"page"},{"location":"client/#Basic-Usage","page":"Client Initialization","title":"Basic Usage","text":"","category":"section"},{"location":"client/","page":"Client Initialization","title":"Client Initialization","text":"using OpenGithubModelsApi\n\n# Create a client with default settings\nclient = GithubModelsClient(\"your_github_token\")\n\n# Create a client with custom settings\nclient = GithubModelsClient(\n    \"your_github_token\",\n    base_url=\"https://models.github.ai\",\n    api_version=\"2022-11-28\"\n)","category":"page"},{"location":"client/#Client-Parameters","page":"Client Initialization","title":"Client Parameters","text":"","category":"section"},{"location":"client/#auth_token-(Required)","page":"Client Initialization","title":"auth_token (Required)","text":"","category":"section"},{"location":"client/","page":"Client Initialization","title":"Client Initialization","text":"Your GitHub authentication token. This token must have appropriate permissions to access the AI models API.","category":"page"},{"location":"client/#base_url-(Optional)","page":"Client Initialization","title":"base_url (Optional)","text":"","category":"section"},{"location":"client/","page":"Client Initialization","title":"Client Initialization","text":"The base URL for the API. Defaults to https://models.github.ai.","category":"page"},{"location":"client/#api_version-(Optional)","page":"Client Initialization","title":"api_version (Optional)","text":"","category":"section"},{"location":"client/","page":"Client Initialization","title":"Client Initialization","text":"The API version to use. Defaults to 2022-11-28.","category":"page"},{"location":"client/#Error-Handling","page":"Client Initialization","title":"Error Handling","text":"","category":"section"},{"location":"client/","page":"Client Initialization","title":"Client Initialization","text":"If invalid parameters are provided, the client constructor will throw appropriate errors:","category":"page"},{"location":"client/","page":"Client Initialization","title":"Client Initialization","text":"Invalid URL format will trigger an ArgumentError\nMissing authentication token will cause a runtime error when making API calls","category":"page"},{"location":"client/#Example","page":"Client Initialization","title":"Example","text":"","category":"section"},{"location":"client/","page":"Client Initialization","title":"Client Initialization","text":"using OpenGithubModelsApi\n\n# Create a client (using a placeholder token for documentation)\nclient = GithubModelsClient(\"ghp_example_token\")\n\nprintln(\"Client created successfully\")\nprintln(\"Base URL: \", client.base_url)\nprintln(\"API Version: \", client.api_version)","category":"page"},{"location":"client/#Best-Practices","page":"Client Initialization","title":"Best Practices","text":"","category":"section"},{"location":"client/","page":"Client Initialization","title":"Client Initialization","text":"Store your authentication token securely (e.g., in environment variables)\nUse the latest stable API version unless you have specific requirements\nHandle potential API errors in your application code","category":"page"},{"location":"client/","page":"Client Initialization","title":"Client Initialization","text":"<div class=\"admonition tip\">\n  <p class=\"admonition-title\">Tip</p>\n  <p>For production applications, consider creating a dedicated GitHub token with only the necessary permissions.</p>\n</div>","category":"page"},{"location":"organization/#Organization-Specific-Features","page":"Organization Features","title":"Organization-Specific Features","text":"","category":"section"},{"location":"organization/","page":"Organization Features","title":"Organization Features","text":"GitHub's AI Models API supports organization-scoped models and access controls.","category":"page"},{"location":"organization/#Example-Usage","page":"Organization Features","title":"Example Usage","text":"","category":"section"},{"location":"organization/","page":"Organization Features","title":"Organization Features","text":"using OpenGithubModelsApi\n\n# Create a client (using a placeholder token for documentation)\nclient = GithubModelsClient(\"ghp_example_token\")\n\n# Create a message history\nmessages = [\n    Message(role=\"user\", content=\"What models does our organization have access to?\")\n]\n\n# Create an inference request\nrequest = InferenceRequest(\n    model=\"publisher/model-name\",  # Replace with an actual model ID\n    messages=messages\n)\n\n# Replace \"your-org\" with your GitHub organization name\nresponse = org_create_chat_completion(client, request, \"your-org\")\nprintln(response)","category":"page"},{"location":"organization/#Organization-Specific-Parameters","page":"Organization Features","title":"Organization-Specific Parameters","text":"","category":"section"},{"location":"organization/","page":"Organization Features","title":"Organization Features","text":"org: The GitHub organization name to scope the request to\nAll standard InferenceRequest parameters apply","category":"page"},{"location":"organization/#Use-Cases","page":"Organization Features","title":"Use Cases","text":"","category":"section"},{"location":"organization/","page":"Organization Features","title":"Organization Features","text":"Organization-scoped API calls are useful for:","category":"page"},{"location":"organization/","page":"Organization Features","title":"Organization Features","text":"Accessing organization-specific models\nEnforcing organization-level security policies\nManaging usage quotas at the organization level\nWorking with organization-specific knowledge bases","category":"page"},{"location":"organization/#Error-Handling","page":"Organization Features","title":"Error Handling","text":"","category":"section"},{"location":"organization/","page":"Organization Features","title":"Organization Features","text":"Common organization-specific errors include:","category":"page"},{"location":"organization/","page":"Organization Features","title":"Organization Features","text":"Organization not found\nInsufficient permissions for the organization\nOrganization-specific rate limits","category":"page"},{"location":"organization/","page":"Organization Features","title":"Organization Features","text":"<div class=\"admonition note\">\n  <p class=\"admonition-title\">Note</p>\n  <p>The authentication token must have appropriate permissions for the specified organization.</p>\n</div>","category":"page"},{"location":"organization/#Best-Practices","page":"Organization Features","title":"Best Practices","text":"","category":"section"},{"location":"organization/","page":"Organization Features","title":"Organization Features","text":"Verify organization name spelling\nEnsure the authentication token has organization access\nHandle organization-specific errors gracefully\nConsider caching organization model information","category":"page"},{"location":"chat_completion/#Chat-Completion-API","page":"Chat Completion","title":"Chat Completion API","text":"","category":"section"},{"location":"chat_completion/","page":"Chat Completion","title":"Chat Completion","text":"The chat completion API allows you to interact with GitHub's AI models through a conversational interface.","category":"page"},{"location":"chat_completion/#Simple-Example","page":"Chat Completion","title":"Simple Example","text":"","category":"section"},{"location":"chat_completion/","page":"Chat Completion","title":"Chat Completion","text":"using OpenGithubModelsApi\n\n# Create a client (using a placeholder token for documentation)\nclient = GithubModelsClient(\"ghp_example_token\")\n\n# Create a message history\nmessages = [\n    Message(role=\"system\", content=\"You are a helpful assistant\"),\n    Message(role=\"user\", content=\"What is Julia programming language?\")\n]\n\n# Create an inference request\nrequest = InferenceRequest(\n    model=\"openai/gpt-4.1\",  # Example model ID\n    messages=messages,\n    temperature=0.7\n)\n\n# Get a response (returns just the content by default)\nresponse = create_chat_completion(client, request)\nprintln(response)","category":"page"},{"location":"chat_completion/#Request-Parameters","page":"Chat Completion","title":"Request Parameters","text":"","category":"section"},{"location":"chat_completion/#Required-Parameters","page":"Chat Completion","title":"Required Parameters","text":"","category":"section"},{"location":"chat_completion/","page":"Chat Completion","title":"Chat Completion","text":"model: ID of the specific model to use (format: {publisher}/{model_name})\nmessages: Array of message objects with role and content","category":"page"},{"location":"chat_completion/","page":"Chat Completion","title":"Chat Completion","text":"Valid message roles are:","category":"page"},{"location":"chat_completion/","page":"Chat Completion","title":"Chat Completion","text":"\"system\": For system instructions\n\"user\": For user messages\n\"assistant\": For assistant responses\n\"developer\": For developer-specific messages","category":"page"},{"location":"chat_completion/#Advanced-Usage","page":"Chat Completion","title":"Advanced Usage","text":"","category":"section"},{"location":"chat_completion/#Getting-Full-Response-Details","page":"Chat Completion","title":"Getting Full Response Details","text":"","category":"section"},{"location":"chat_completion/","page":"Chat Completion","title":"Chat Completion","text":"By default, create_chat_completion returns just the content string. To get the full response object, use verbose=true:","category":"page"},{"location":"chat_completion/","page":"Chat Completion","title":"Chat Completion","text":"full_response = create_chat_completion(client, request, verbose=true)","category":"page"},{"location":"chat_completion/#Controlling-Response-Length-and-Creativity","page":"Chat Completion","title":"Controlling Response Length and Creativity","text":"","category":"section"},{"location":"chat_completion/","page":"Chat Completion","title":"Chat Completion","text":"request = InferenceRequest(\n    model=\"openai/gpt-4.1\",\n    messages=messages,\n    temperature=0.3,      # More deterministic (0.0-1.0)\n    max_tokens=100,       # Limit response length\n    top_p=0.9             # Alternative to temperature\n)","category":"page"},{"location":"chat_completion/#Error-Handling","page":"Chat Completion","title":"Error Handling","text":"","category":"section"},{"location":"chat_completion/","page":"Chat Completion","title":"Chat Completion","text":"Common errors include:","category":"page"},{"location":"chat_completion/","page":"Chat Completion","title":"Chat Completion","text":"Invalid model ID format\nInvalid message roles\nParameter validation errors (e.g., temperature outside 0-1 range)\nAPI rate limits","category":"page"},{"location":"chat_completion/","page":"Chat Completion","title":"Chat Completion","text":"<div class=\"admonition warning\">\n  <p class=\"admonition-title\">Warning</p>\n  <p>Streaming responses are not supported by this package. Setting <code>stream=true</code> will result in an error.</p>\n</div>","category":"page"},{"location":"chat_completion/#Best-Practices","page":"Chat Completion","title":"Best Practices","text":"","category":"section"},{"location":"chat_completion/","page":"Chat Completion","title":"Chat Completion","text":"Always include a system message to guide the model's behavior\nKeep conversation history within the model's context window\nAdjust temperature based on your use case (lower for factual responses, higher for creative tasks)\nValidate inputs before making API calls","category":"page"},{"location":"models/#Model-Catalog","page":"Model Catalog","title":"Model Catalog","text":"","category":"section"},{"location":"models/","page":"Model Catalog","title":"Model Catalog","text":"The model catalog API allows you to discover and understand available AI models.","category":"page"},{"location":"models/#Basic-Usage","page":"Model Catalog","title":"Basic Usage","text":"","category":"section"},{"location":"models/","page":"Model Catalog","title":"Model Catalog","text":"using OpenGithubModelsApi\n\n# Create a client (using a placeholder token for documentation)\nclient = GithubModelsClient(\"ghp_example_token\")\n\n# Get all available models\nmodels = list_models(client)\n\n# Print model information\nfor model in models\n    println(\"Model: $(model.name) (ID: $(model.id))\")\n    println(\"Publisher: $(model.publisher)\")\n    println(\"Summary: $(model.summary)\")\n    println(\"---\")\nend","category":"page"},{"location":"models/#Key-Model-Properties","page":"Model Catalog","title":"Key Model Properties","text":"","category":"section"},{"location":"models/","page":"Model Catalog","title":"Model Catalog","text":"id: Unique model identifier (format: {publisher}/{model_name})\nname: Human-readable model name\npublisher: Organization or individual who published the model\nsummary: Brief description of the model's capabilities\nrate_limit_tier: Rate limiting tier for the model\ntags: Categorization tags for the model\nsupported_input_modalities: Input formats the model accepts\nsupported_output_modalities: Output formats the model produces","category":"page"},{"location":"models/#Filtering-Models","page":"Model Catalog","title":"Filtering Models","text":"","category":"section"},{"location":"models/","page":"Model Catalog","title":"Model Catalog","text":"While the API returns all available models, you can filter them in Julia:","category":"page"},{"location":"models/","page":"Model Catalog","title":"Model Catalog","text":"# Get only text models\ntext_models = filter(m -> \"text\" in m.supported_input_modalities, models)\n\n# Get models from a specific publisher\nopenai_models = filter(m -> m.publisher == \"openai\", models)","category":"page"},{"location":"models/#Model-Selection-Guidance","page":"Model Catalog","title":"Model Selection Guidance","text":"","category":"section"},{"location":"models/","page":"Model Catalog","title":"Model Catalog","text":"When selecting a model, consider:","category":"page"},{"location":"models/","page":"Model Catalog","title":"Model Catalog","text":"The input and output modalities you need\nThe model's capabilities as described in the summary\nThe rate limit tier for your expected usage volume\nThe publisher's reputation and support","category":"page"},{"location":"models/","page":"Model Catalog","title":"Model Catalog","text":"<div class=\"admonition tip\">\n  <p class=\"admonition-title\">Tip</p>\n  <p>Check the model's supported modalities to ensure compatibility with your application's requirements.</p>\n</div>","category":"page"},{"location":"models/#Best-Practices","page":"Model Catalog","title":"Best Practices","text":"","category":"section"},{"location":"models/","page":"Model Catalog","title":"Model Catalog","text":"Cache model listings to reduce API calls\nValidate model IDs before using them in requests\nHandle cases where expected models might not be available\nConsider creating a model registry for your application","category":"page"},{"location":"troubleshooting/#Troubleshooting","page":"Troubleshooting","title":"Troubleshooting","text":"","category":"section"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"This section covers common issues and their solutions when using the OpenGithubModelsApi.jl package.","category":"page"},{"location":"troubleshooting/#Common-Errors","page":"Troubleshooting","title":"Common Errors","text":"","category":"section"},{"location":"troubleshooting/#Authentication-Errors","page":"Troubleshooting","title":"Authentication Errors","text":"","category":"section"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"Symptom: 401 Unauthorized errors","category":"page"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"Solution:","category":"page"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"Verify your GitHub token has the necessary permissions\nCheck that the token hasn't expired\nEnsure you're using the correct token format (ghp_...)","category":"page"},{"location":"troubleshooting/#Invalid-Model-ID-Format","page":"Troubleshooting","title":"Invalid Model ID Format","text":"","category":"section"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"Symptom: Error about model ID format","category":"page"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"Solution:","category":"page"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"Model IDs must be in the format {publisher}/{model_name}\nVerify the model exists using list_models()\nCheck for typos in the model ID","category":"page"},{"location":"troubleshooting/#Parameter-Validation-Errors","page":"Troubleshooting","title":"Parameter Validation Errors","text":"","category":"section"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"Symptom: Errors about invalid parameters","category":"page"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"Common issues:","category":"page"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"Temperature outside 0-1 range\nInvalid message roles\nUnsupported modalities","category":"page"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"Solution:","category":"page"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"Check parameter constraints in the documentation\nValidate inputs before making API calls\nUse the type-safe constructors to catch errors early","category":"page"},{"location":"troubleshooting/#Streaming-Not-Supported","page":"Troubleshooting","title":"Streaming Not Supported","text":"","category":"section"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"Symptom: \"Error: Streaming Not Supported\" message","category":"page"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"Solution:","category":"page"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"This package does not support streaming responses\nSet stream=false or omit the parameter (default is false)","category":"page"},{"location":"troubleshooting/#Debugging-Tips","page":"Troubleshooting","title":"Debugging Tips","text":"","category":"section"},{"location":"troubleshooting/#Enable-Verbose-Mode","page":"Troubleshooting","title":"Enable Verbose Mode","text":"","category":"section"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"For more detailed response information:","category":"page"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"response = create_chat_completion(client, request, verbose=true)","category":"page"},{"location":"troubleshooting/#Inspect-Model-Data","page":"Troubleshooting","title":"Inspect Model Data","text":"","category":"section"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"To understand available models:","category":"page"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"models = list_models(client)\nfor model in models\n    println(\"Model ID: $(model.id)\")\n    println(\"Supported input modalities: $(model.supported_input_modalities)\")\n    println(\"Supported output modalities: $(model.supported_output_modalities)\")\n    println(\"---\")\nend","category":"page"},{"location":"troubleshooting/#Validate-Request-Before-Sending","page":"Troubleshooting","title":"Validate Request Before Sending","text":"","category":"section"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"try\n    # This will validate the request structure\n    request = InferenceRequest(\n        model=\"openai/gpt-4.1\",\n        messages=[Message(role=\"user\", content=\"Hello\")],\n        temperature=0.5\n    )\n    # If we get here, the request is valid\n    response = create_chat_completion(client, request)\ncatch e\n    println(\"Request validation failed: $e\")\nend","category":"page"},{"location":"troubleshooting/#Frequently-Asked-Questions","page":"Troubleshooting","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"troubleshooting/#Q:-Why-am-I-getting-\"Invalid-message-role\"-errors?","page":"Troubleshooting","title":"Q: Why am I getting \"Invalid message role\" errors?","text":"","category":"section"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"A: Message roles must be one of: \"assistant\", \"developer\", \"system\", or \"user\". Check your message definitions for typos.","category":"page"},{"location":"troubleshooting/#Q:-Can-I-use-streaming-responses-with-this-package?","page":"Troubleshooting","title":"Q: Can I use streaming responses with this package?","text":"","category":"section"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"A: No, streaming responses are explicitly not supported by this package. Attempting to set stream=true will result in an error.","category":"page"},{"location":"troubleshooting/#Q:-How-do-I-find-available-models?","page":"Troubleshooting","title":"Q: How do I find available models?","text":"","category":"section"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"A: Use the list_models(client) function to retrieve a list of all available models with their metadata.","category":"page"},{"location":"troubleshooting/#Q:-What's-the-difference-between-create_chat_completion-and-org_create_chat_completion?","page":"Troubleshooting","title":"Q: What's the difference between create_chat_completion and org_create_chat_completion?","text":"","category":"section"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"A: org_create_chat_completion is for organization-scoped requests and requires an additional organization name parameter, while create_chat_completion is for standard requests.","category":"page"},{"location":"troubleshooting/#Reporting-Issues","page":"Troubleshooting","title":"Reporting Issues","text":"","category":"section"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"If you encounter problems that aren't covered here:","category":"page"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"Check the GitHub issues page\nInclude:\nPackage version\nJulia version\nCode snippet that reproduces the issue\nFull error message\nOpen a new issue with the details","category":"page"},{"location":"troubleshooting/","page":"Troubleshooting","title":"Troubleshooting","text":"<div class=\"admonition tip\">\n  <p class=\"admonition-title\">Tip</p>\n  <p>When reporting issues, include the output of <code>versioninfo()</code> and the package status to help diagnose the problem.</p>\n</div>","category":"page"},{"location":"models_reference/#Data-Models-Reference","page":"Data Models Reference","title":"Data Models Reference","text":"","category":"section"},{"location":"models_reference/","page":"Data Models Reference","title":"Data Models Reference","text":"This section provides detailed documentation for all data structures used in the API.","category":"page"},{"location":"models_reference/#OpenGithubModelsApi.Function_Params","page":"Data Models Reference","title":"OpenGithubModelsApi.Function_Params","text":"Function_Params\n\nFunction_Params(;\n    name=nothing,\n    description=nothing,\n    parameters=nothing,\n)\n\n- name::String : The name of the function to be called.\n- description::String : A description of what the function does. The model will use this description when selecting the function and interpreting its parameters.\n- parameters::A Json Object that describes the function params\n\n\n\n\n\n","category":"type"},{"location":"models_reference/#OpenGithubModelsApi.InferenceRequest","page":"Data Models Reference","title":"OpenGithubModelsApi.InferenceRequest","text":"InferenceRequest\n\nInferenceRequest(;\n    model=nothing,\n    messages=nothing,\n    frequency_penalty=nothing,\n    max_tokens=nothing,\n    modalities=nothing,\n    presence_penalty=nothing,\n    response_format=nothing,\n    seed=nothing,\n    stream=false,\n    stream_options=nothing,\n    stop=nothing,\n    temperature=nothing,\n    tool_choice=nothing,\n    tools=nothing,\n    top_p=nothing,\n)\n\n- model::String : ID of the specific model to use for the request.\n- messages::Vector{Message} : The collection of context messages associated with this chat completion request. Typical usage begins with a chat message for the System role that provides instructions for the behavior of the assistant, followed by alternating messages between the User and Assistant roles.\n- frequency_penalty::Float64 : A value that influences the probability of generated tokens appearing based on their cumulative frequency in generated text. Positive values will make tokens less likely to appear as their frequency increases and decrease the likelihood of the model repeating the same statements verbatim. Supported range is [-2, 2].\n- max_tokens::Int64 : The maximum number of tokens to generate in the completion. The token count of your prompt plus max_tokens cannot exceed the model&#39;s context length. For example, if your prompt is 100 tokens and you set max_tokens to 50, the API will return a completion with a maximum of 50 tokens.\n- modalities::Vector{String} : The modalities that the model is allowed to use for the chat completions response. The default modality is text. Indicating an unsupported modality combination results in a 422 error. Supported values are: text, audio \n- presence_penalty::Float64 : A value that influences the probability of generated tokens appearing based on their existing presence in generated text. Positive values will make tokens less likely to appear when they already exist and increase the model&#39;s likelihood to output new tokens. Supported range is [-2, 2].\n- response_format::InferenceRequestResponseFormat\n- seed::Int64 : If specified, the system will make a best effort to sample deterministically such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed.\n- stream::Bool : A value indicating whether chat completions should be streamed for this request.\n- stream_options::InferenceRequestStreamOptions\n- stop::Vector{String} : A collection of textual sequences that will end completion generation.\n- temperature::Float64 : The sampling temperature to use that controls the apparent creativity of generated completions. Higher values will make output more random while lower values will make results more focused and deterministic. It is not recommended to modify temperature and top_p for the same completion request as the interaction of these two settings is difficult to predict. Supported range is [0, 1]. Decimal values are supported.\n- tool_choice::String : If specified, the model will configure which of the provided tools it can use for the chat completions response.\n- tools::Vector{InferenceRequestToolsInner} : A list of tools the model may request to call. Currently, only functions are supported as a tool. The model may respond with a function call request and provide the input arguments in JSON format for that function.\n- top_p::Float64 : An alternative to sampling with temperature called nucleus sampling. This value causes the model to consider the results of tokens with the provided probability mass. As an example, a value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be considered. It is not recommended to modify temperature and top_p for the same request as the interaction of these two settings is difficult to predict. Supported range is [0, 1]. Decimal values are supported.\n\n\n\n\n\n","category":"type"},{"location":"models_reference/#OpenGithubModelsApi.InferenceRequestResponseFormat","page":"Data Models Reference","title":"OpenGithubModelsApi.InferenceRequestResponseFormat","text":"InferenceRequestresponseformat\n\nInferenceRequestResponseFormat(;\n    type=nothing,\n    json_schema=nothing,\n)\n\n- type::String : The type of the response.\n- json_schema::Any : The JSON schema for the response.\n\n\n\n\n\n","category":"type"},{"location":"models_reference/#OpenGithubModelsApi.InferenceRequestStreamOptions","page":"Data Models Reference","title":"OpenGithubModelsApi.InferenceRequestStreamOptions","text":"InferenceRequeststreamoptions\n\nInferenceRequestStreamOptions(;\n    include_usage=false,\n)\n\n- include_usage::Bool : Whether to include usage information in the response.\n\n\n\n\n\n","category":"type"},{"location":"models_reference/#OpenGithubModelsApi.InferenceRequestToolsInner","page":"Data Models Reference","title":"OpenGithubModelsApi.InferenceRequestToolsInner","text":"InferenceRequesttoolsinner\n\nInferenceRequestToolsInner(;\n    call_function=nothing,\n    type=nothing,\n)\n\n- call_function::Function_Params\n- type::String\n\n\n\n\n\n","category":"type"},{"location":"models_reference/#OpenGithubModelsApi.InferenceResponse","page":"Data Models Reference","title":"OpenGithubModelsApi.InferenceResponse","text":"InferenceResponse\n\nInferenceResponse(;\n    choices=nothing,\n    data=nothing,\n)\n\n- choices::Vector{NonStreamingResponseChoices}\n- data::StreamingResponseData\n\n\n\n\n\n","category":"type"},{"location":"models_reference/#OpenGithubModelsApi.Message","page":"Data Models Reference","title":"OpenGithubModelsApi.Message","text":"InferenceRequestmessagesinner\n\nMessage(;\n    role=nothing,\n    content=nothing,\n)\n\n- role::String : The chat role associated with this message\n- content::String : The content of the message\n\n\n\n\n\n","category":"type"},{"location":"models_reference/#OpenGithubModelsApi.ModelData","page":"Data Models Reference","title":"OpenGithubModelsApi.ModelData","text":"ModelData\n\nModelData(;\n    id=nothing,\n    name=nothing,\n    publisher=nothing,\n    summary=nothing,\n    rate_limit_tier=nothing,\n    tags=nothing,\n    supported_input_modalities=nothing,\n    supported_output_modalities=nothing,\n)\n\n- id::String : The unique identifier for the model\n- name::String : The name of the model\n- publisher::String : The publisher of the model\n- summary::String : A brief summary of the model&#39;s capabilities\n- rate_limit_tier::String : The rate limit tier for the model\n- tags::Vector{String} : A list of tags associated with the model\n- supported_input_modalities::Vector{String} : A list of input modalities supported by the model\n- supported_output_modalities::Vector{String} : A list of output modalities supported by the model\n\n\n\n\n\n","category":"type"},{"location":"models_reference/#OpenGithubModelsApi.NonStreamingResponseChoices","page":"Data Models Reference","title":"OpenGithubModelsApi.NonStreamingResponseChoices","text":"NonStreamingResponse_choices\n\nNonStreamingResponseChoices(;\n    message=nothing,\n)\n\n- message::NonStreamingResponseMessage\n\n\n\n\n\n","category":"type"},{"location":"models_reference/#OpenGithubModelsApi.NonStreamingResponseMessage","page":"Data Models Reference","title":"OpenGithubModelsApi.NonStreamingResponseMessage","text":"NonStreamingResponse_message The message associated with the completion.\n\nNonStreamingResponseMessage(;\n    content=nothing,\n    role=nothing,\n)\n\n- content::String : The content of the message.\n- role::String : The role of the message.\n\n\n\n\n\n","category":"type"},{"location":"models_reference/#OpenGithubModelsApi.StreamingResponseData","page":"Data Models Reference","title":"OpenGithubModelsApi.StreamingResponseData","text":"StreamingResponsedata Some details about the response.\n\nStreamingResponseData(;\n    choices=nothing,\n)\n\n- choices::Vector{StreamingResponseDataChoices}\n\n\n\n\n\n","category":"type"},{"location":"models_reference/#OpenGithubModelsApi.StreamingResponseDataChoices","page":"Data Models Reference","title":"OpenGithubModelsApi.StreamingResponseDataChoices","text":"StreamingResponsedata_choices\n\nStreamingResponseDataChoices(;\n    delta=nothing,\n)\n\n- delta::StreamingResponseDataDelta\n\n\n\n\n\n","category":"type"},{"location":"models_reference/#OpenGithubModelsApi.StreamingResponseDataDelta","page":"Data Models Reference","title":"OpenGithubModelsApi.StreamingResponseDataDelta","text":"StreamingResponsedata_delta Container for the content of the streamed response.\n\nStreamingResponseDataDelta(;\n    content=nothing,\n)\n\n- content::String : The content of the streamed response.\n\n\n\n\n\n","category":"type"},{"location":"models_reference/#OpenGithubModelsApi.create_chat_completion-Tuple{GithubModelsClient, InferenceRequest}","page":"Data Models Reference","title":"OpenGithubModelsApi.create_chat_completion","text":"Creates a chat completion.\n\nParams:\n\nauth_token::String (required)\napi_version::String (required)\ninference_request::InferenceRequest (required)\n\nReturn: InferenceResponse, OpenAPI.Clients.ApiResponse\n\n\n\n\n\n","category":"method"},{"location":"models_reference/#OpenGithubModelsApi.create_org_chat_completion-Tuple{GithubModelsClient, String, InferenceRequest}","page":"Data Models Reference","title":"OpenGithubModelsApi.create_org_chat_completion","text":"Creates a chat completion for a given organization.\n\nParams:\n\norg::String (required)\nauth_token::String (required)\napi_version::String (required)\ninference_request::InferenceRequest (required)\n\nReturn: InferenceResponse, OpenAPI.Clients.ApiResponse\n\n\n\n\n\n","category":"method"},{"location":"models_reference/#OpenGithubModelsApi.list_models-Tuple{GithubModelsClient}","page":"Data Models Reference","title":"OpenGithubModelsApi.list_models","text":"Lists available models.\n\nParams:\n\nauth_token::String (required)\napi_version::String (required)\n\nReturn: Vector{ModelData}, OpenAPI.Clients.ApiResponse\n\n\n\n\n\n","category":"method"},{"location":"models_reference/#Parameter-Validation-Rules","page":"Data Models Reference","title":"Parameter Validation Rules","text":"","category":"section"},{"location":"models_reference/#Temperature-and-Top-P","page":"Data Models Reference","title":"Temperature and Top P","text":"","category":"section"},{"location":"models_reference/","page":"Data Models Reference","title":"Data Models Reference","text":"Range: 0.0 to 1.0\nDecimal values are supported\nNot recommended to modify both simultaneously","category":"page"},{"location":"models_reference/#Message-Roles","page":"Data Models Reference","title":"Message Roles","text":"","category":"section"},{"location":"models_reference/","page":"Data Models Reference","title":"Data Models Reference","text":"Valid values: \"assistant\", \"developer\", \"system\", \"user\"","category":"page"},{"location":"models_reference/#Tool-Choice","page":"Data Models Reference","title":"Tool Choice","text":"","category":"section"},{"location":"models_reference/","page":"Data Models Reference","title":"Data Models Reference","text":"Valid values: \"auto\", \"required\", \"none\"","category":"page"},{"location":"models_reference/#Modalities","page":"Data Models Reference","title":"Modalities","text":"","category":"section"},{"location":"models_reference/","page":"Data Models Reference","title":"Data Models Reference","text":"Supported values: \"text\", \"audio\"","category":"page"},{"location":"models_reference/","page":"Data Models Reference","title":"Data Models Reference","text":"<div class=\"admonition warning\">\n  <p class=\"admonition-title\">Warning</p>\n  <p>Setting <code>stream=true</code> is not supported and will result in an error.</p>\n</div>","category":"page"},{"location":"models_reference/#Best-Practices-for-Data-Models","page":"Data Models Reference","title":"Best Practices for Data Models","text":"","category":"section"},{"location":"models_reference/","page":"Data Models Reference","title":"Data Models Reference","text":"Always validate required fields before making API calls\nUse the provided type constraints to ensure valid parameters\nHandle optional fields appropriately in your application\nBe aware of enum constraints for string parameters","category":"page"},{"location":"","page":"Home","title":"Home","text":"OpenGithubModelsApi.jl is a Julia client library for interacting with GitHub's AI Models API (https://models.github.ai). This package was automatically generated using OpenAPI.jl, providing a type-safe and validated interface to GitHub's machine learning models.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Type-safe API client for GitHub's AI models\nSecure authentication with GitHub tokens\nChat completion functionality\nOrganization-specific model access\nModel catalog listing\nComprehensive parameter validation\nWell-structured Julia package with proper documentation","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the package, use Julia's package manager:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"OpenGithubModelsApi\")","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using OpenGithubModelsApi\n\n# Create a client with your GitHub token\nclient = GithubModelsClient(\"YOUR_GITHUB_TOKEN\")\n\n# Create a message history\nmessages = [\n    Message(role=\"system\", content=\"You are a helpful assistant\"),\n    Message(role=\"user\", content=\"What is Julia programming language?\")\n]\n\n# Create an inference request\nrequest = InferenceRequest(\n    model=\"publisher/model-name\",  # Replace with an actual model ID\n    messages=messages,\n    temperature=0.7\n)\n\n# Get a response\nresponse = create_chat_completion(client, request)\nprintln(response)","category":"page"},{"location":"#Documentation-Structure","page":"Home","title":"Documentation Structure","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This documentation is organized into several sections:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Client Initialization - How to set up the API client\nChat Completion- Using the standard chat completion API\nOrganization Features - Working with organization-specific models\nModel Catalog - Listing and understanding available models\nData Models Reference - Detailed reference for all data structures\nTroubleshooting - Common issues and solutions","category":"page"},{"location":"#Why-Use-This-Package?","page":"Home","title":"Why Use This Package?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides a Julia-native interface to GitHub's AI models API with several advantages over direct API calls:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Type Safety: All API parameters and responses are strongly typed\nValidation: Automatic validation of parameters according to GitHub's API specifications\nError Handling: Consistent error handling patterns\nSimplified Interface: High-level functions that handle common use cases\nDocumentation: Comprehensive documentation for all API endpoints","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div class=\"admonition note\">\n  <p class=\"admonition-title\">Note</p>\n  <p>This package was automatically generated using OpenAPI.jl from GitHub's API specification.</p>\n</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div class=\"admonition warning\">\n  <p class=\"admonition-title\">Warning</p>\n  <p>Streaming responses are not currently supported by this package. Attempting to use streaming will result in an error.</p>\n</div>","category":"page"}]
}
