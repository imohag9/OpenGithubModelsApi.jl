# This file was generated by the Julia OpenAPI Code Generator
# Do not modify this file directly. Modify the OpenAPI specification instead.

struct DefaultApi <: OpenAPI.APIClientImpl
    client::OpenAPI.Clients.Client
end

"""
The default API base path for APIs in `DefaultApi`.
This can be used to construct the `OpenAPI.Clients.Client` instance.
"""
basepath(::Type{ DefaultApi }) = "https://models.github.ai"

const _returntypes_create_chat_completion_DefaultApi = Dict{Regex,Type}(
    Regex("^" * replace("200", "x"=>".") * "\$") => InferenceResponse,
)

function _oacinternal_create_chat_completion(_api::DefaultApi, auth_token::String, api_version::String, inference_request::InferenceRequest; _mediaType=nothing)
    _ctx = OpenAPI.Clients.Ctx(_api.client, "POST", _returntypes_create_chat_completion_DefaultApi, "/inference/chat/completions", [], inference_request)
    OpenAPI.Clients.set_param(_ctx.header, "Authorization", "Bearer $(auth_token)")  # type String
    OpenAPI.Clients.set_param(_ctx.header, "X-GitHub-Api-Version", api_version)  # type String
    OpenAPI.Clients.set_header_accept(_ctx, ["application/vnd.github+json", ])
    OpenAPI.Clients.set_header_content_type(_ctx, (_mediaType === nothing) ? ["application/json", ] : [_mediaType])
    return _ctx
end

@doc raw"""Creates a chat completion.

Params:
- auth_token::String (required)
- api_version::String (required)
- inference_request::InferenceRequest (required)

Return: InferenceResponse, OpenAPI.Clients.ApiResponse
"""
function create_chat_completion(_api::DefaultApi, auth_token::String, api_version::String, inference_request::InferenceRequest; _mediaType=nothing)
    _ctx = _oacinternal_create_chat_completion(_api, auth_token, api_version, inference_request; _mediaType=_mediaType)
    inference_request === true && return "Error: Streaming Not Supported "
    return OpenAPI.Clients.exec(_ctx)
end

function create_chat_completion(_api::DefaultApi, response_stream::Channel, auth_token::String, api_version::String, inference_request::InferenceRequest; _mediaType=nothing)
    _ctx = _oacinternal_create_chat_completion(_api, auth_token, api_version, inference_request; _mediaType=_mediaType)
    inference_request === true && return "Error: Streaming Not Supported "

    return OpenAPI.Clients.exec(_ctx, response_stream)
end

const _returntypes_create_org_chat_completion_DefaultApi = Dict{Regex,Type}(
    Regex("^" * replace("200", "x"=>".") * "\$") => InferenceResponse,
)

function _oacinternal_create_org_chat_completion(_api::DefaultApi, org::String, auth_token::String, api_version::String, inference_request::InferenceRequest; _mediaType=nothing)
    _ctx = OpenAPI.Clients.Ctx(_api.client, "POST", _returntypes_create_org_chat_completion_DefaultApi, "/orgs/{org}/inference/chat/completions", [], inference_request)
    OpenAPI.Clients.set_param(_ctx.path, "org", org)  # type String
    OpenAPI.Clients.set_param(_ctx.header, "Authorization", "Bearer $(auth_token)")  # type String
    OpenAPI.Clients.set_param(_ctx.header, "X-GitHub-Api-Version", api_version)  # type String
    OpenAPI.Clients.set_header_accept(_ctx, ["application/vnd.github+json", ])
    OpenAPI.Clients.set_header_content_type(_ctx, (_mediaType === nothing) ? ["application/json", ] : [_mediaType])
    return _ctx
end

@doc raw"""Creates a chat completion for a given organization.

Params:
- org::String (required)
- auth_token::String (required)
- api_version::String (required)
- inference_request::InferenceRequest (required)

Return: InferenceResponse, OpenAPI.Clients.ApiResponse
"""
function create_org_chat_completion(_api::DefaultApi, org::String, auth_token::String, api_version::String, inference_request::InferenceRequest; _mediaType=nothing)

    _ctx = _oacinternal_create_org_chat_completion(_api, org, auth_token, api_version, inference_request; _mediaType=_mediaType)
    inference_request === true && return "Error: Streaming Not Supported "

    return OpenAPI.Clients.exec(_ctx)
end

function create_org_chat_completion(_api::DefaultApi, response_stream::Channel, org::String, auth_token::String, api_version::String, inference_request::InferenceRequest; _mediaType=nothing)
    _ctx = _oacinternal_create_org_chat_completion(_api, org, auth_token, api_version, inference_request; _mediaType=_mediaType)
    inference_request === true && return "Error: Streaming Not Supported "

    return OpenAPI.Clients.exec(_ctx, response_stream)
end

const _returntypes_list_models_DefaultApi = Dict{Regex,Type}(
    Regex("^" * replace("200", "x"=>".") * "\$") => Vector{ModelData},
)

function _oacinternal_list_models(_api::DefaultApi, auth_token::String, api_version::String; _mediaType=nothing)
    _ctx = OpenAPI.Clients.Ctx(_api.client, "GET", _returntypes_list_models_DefaultApi, "/catalog/models", [])
    OpenAPI.Clients.set_param(_ctx.header, "Authorization", "Bearer $(auth_token)")  # type String
    OpenAPI.Clients.set_param(_ctx.header, "X-GitHub-Api-Version", api_version)  # type String
    OpenAPI.Clients.set_header_accept(_ctx, ["application/vnd.github+json", ])
    OpenAPI.Clients.set_header_content_type(_ctx, (_mediaType === nothing) ? [] : [_mediaType])
    return _ctx
end

@doc raw"""Lists available models.

Params:
- auth_token::String (required)
- api_version::String (required)

Return: Vector{ModelData}, OpenAPI.Clients.ApiResponse
"""
function list_models(_api::DefaultApi, auth_token::String, api_version::String; _mediaType=nothing)
    _ctx = _oacinternal_list_models(_api, auth_token, api_version; _mediaType=_mediaType)
    return OpenAPI.Clients.exec(_ctx)
end

function list_models(_api::DefaultApi, response_stream::Channel, auth_token::String, api_version::String; _mediaType=nothing)
    _ctx = _oacinternal_list_models(_api, auth_token, api_version; _mediaType=_mediaType)
    return OpenAPI.Clients.exec(_ctx, response_stream)
end

export create_chat_completion
export create_org_chat_completion
export list_models
